<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Language Models with Rationality</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ai2_website_top.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Language Models with Rationality</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://norakassner.github.io/">Nora Kassner</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://allenai.org/team/oyvindt">Oyvind Tafjord</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://allenai.org/team/ashishs">Ashish Sabharwal</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://allenai.org/team/kyler">Kyle Richardson</a><sup>1</sup>,
            </span>	    
            <span class="author-block">
	      <a href="https://scholar.google.com/citations?user=qIL9dWUAAAAJ&hl=en">Hinrich Sch√ºtze</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://allenai.org/team/peterc">Peter Clark</a><sup>1</sup>.
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Allen Institute for AI,</span>
            <span class="author-block"><sup>2</sup>LMU Munich, Germany</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">      
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="100%" height="100%">
        <source src="./static/videos/reflex.mp4"
                type="video/mp4">
      </video>
    </div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
	    While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent "beliefs". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a <b>rational, self-reflecting layer</b> on top of the LLM. First, given a question, we construct a <b>belief graph</b> using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.
          </p>
	  <h3 class="title is-4">Overview</h3>
	  <p>
While large language models (LLMs) are impressive at question-answering (QA), it is
not always clear how (or even if) an answer follows from their latent "beliefs"
about the world, or whether the LLM even has a coherent internal belief system.
This general opacity is a growing impediment to widespread use of LLMs, e.g.,
in critical applications such as medicine, law, and hiring decisions,
where properties of explainability, interpretability, and trust are
paramount. Our goal is to help alleviate such opacity by
constructing an explicit representation of 
system beliefs and their inferential relationships (including to answer candidates),
so that answers are supported by interpretable chains of reasoning.
These constructed <b>belief graphs</b> form a <b>rational layer</b> above the LLM explaining how answers follow from beliefs,
and provide a window into some of the latent contents of the model, potentially
helping users understand and trust model answers.
</p><p>
In addition, when we do this, we find such graphs expose latent inconsistencies
in the model's beliefs. We show how such inconsistencies can be resolved 
using constraint satisfaction techniques.
When we do this, the rational layer becomes not just a window onto
the model, but an active reasoning component in its own right in a <i>larger, overall
system</i>, comprising the (frozen) LLM plus rational layer. We show
this results in a more consistent set of beliefs in the overall system,
without harming overall answer accuracy (although some individual answers
may change). The result is answers supported by faithful, system-believed
chains of reasoning drawn from a consistent belief system. 
</p>
	  <h3 class="title is-4">Approach</h3>
    <div class="columns is-centered has-text-centered">	  
      <img src="./static/images/graph-growth.png" display="block" style="max-width: 80%; align-items: center; ">
      </div>
          <p>
	    <ul>
<li> <b>A:</b> Given a question, each answer choice is first converted to a hypothesis statement.
<li> <b>B:</b> The belief graph is then constructed in stages, first generating rules that conclude the hypotheses,
<li> <b>C:</b> then backward-chaining to generate rules concluding the premises of those first rules, etc., 
and adding in negated versions of graph statements connected with the originals via XOR links (e.g., nodes 11 and 12),
  until the stopping criterion is met.
<li> Statements are then labeled with the model's belief in them (true/false),
  found via self-querying (white = believed true, grey = believed false).
  <li> Finally, logical conflicts are identified
(colored red), and constraint satisfaction techniques are used to resolve them.
In this case, as there is strong evidence that node 2 is actually true (7 & 6 -> 2, not(19) -> 2),
the solver finds that the minimum cost repair is to flip node 2's label from FALSE to TRUE. Here, node 2 ends up being
selected as the final answer, thus correctly answering the original question.
	    </ul>
          </p>
	  <h3 class="title is-4">Results</h3>	  

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Consistency</h2>
          <img src="./static/images/consistency.png" display="block" style="max-width: 100%; align-items: center; margin: auto;">
          <p>
	    By adding a rational layer to the baseline LLM, REFLEX
    significantly improves consistency among beliefs by resolving uncovered conflicts.
          </p>
	  
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
          <h2 class="title is-4">Accuracy</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/accuracy.png" display="block" style="max-width: 100%; align-items: center; margin: auto;">
	    <p>
	      REFLEX's belief revision in the rational layer preserves overall QA accuracy.
	      </p>
          </div>
        </div>
      </div>
    </div>
    
	  <h3 class="title is-4">Conclusion</h3>
          <p>
REFLEX is an approach in which an explicit, interpretable representation of their beliefs and their relationships
is constructed, and which can be reasoned over.
It adds a <b>rational layer</b> above the LLM, 
providing a window into system beliefs, and allowing
latent inconsistencies in the LLM alone to be repaired.
Our implementation shows that belief consistency of the overall system is
significantly improved, without harming answer accuracy,
resulting in answers supported by interpretable chains of reasoning drawn
from a more consistent belief system. This is an important step
towards improving confidence in system behavior, and
towards trustable deployment of LLMs in practical applications.
</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{Kassner2023LanguageMW,
  title={Language Models with Rationality},
  author={Nora Kassner and Oyvind Tafjord and Ashish Sabharwal and Kyle Richardson and Hinrich Sch{\"u}tze and Peter Clark},
  booktitle={EMNLP},
  year={2023}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <div class="publication-links">
      <!-- <div class="column is-8"> -->
        <span class="link-block">
          <img src="./static/images/ai2-logo-header.png" display="block" style="max-width: 20%; align-items: center;">
        </span>
        <br>
        <span class="link-block">
          <img src="./static/images/aristo-logo-header.png" display="block" style="max-width: 15%; align-items: center;">
        </span>
        </div>
      <!-- </div> -->
    </div>
    </div>
    <br>
    <p><center><a href="https://allenai.org/">Allen Institute for AI</a> - all rights reserved.<br>
      The website template is borrowed from <a href="https://nerfies.github.io/">here</a>.</center></p>
    </div>
  </div>
</footer>

</body>
</html>
